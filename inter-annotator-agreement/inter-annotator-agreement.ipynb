{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inter-annotator Agreement for INLG 2020\n",
    "\n",
    "The annotation team formalized our annotation guidelines in July 2020.\n",
    "We then annotated 10 papers from ACL 2020 according to these new guidelines to get an idea of the degree of inter-annotator agreement;\n",
    "this gives us a sense of how reliable the new guidelines and our annotations are.\n",
    "\n",
    "In this notebook, we import data from our spreadsheets and do a bit of preprocessing so that we can calculate IAA easily using `nltk`.\n",
    "\n",
    "We calculate [Krippendorff's alpha]() using [MASI distance]() and [Jaccard distance]().\n",
    "We also include raw pair-wise agreement scores.\n",
    "\n",
    "Note that we are not doing any hypothesis testing here, so you will not see any significance scores.\n",
    "These are strictly descriptive statistics.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Our original annotations were collected using [Google Sheets]() so we used `gspread` to interact with Google Sheets, `nltk` to calculate $\\alpha$, and `pandas` to manage our data.\n",
    "These spreadsheets are not public, but the data from them is released in the CSV files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import edit_distance, jaccard_distance, masi_distance\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import iaa_utilities\n",
    "\n",
    "\n",
    "# URLs for the actual spreadsheets\n",
    "# NB: These spreadsheets are not public, but the data is released in the included CSV files.\n",
    "urls = [\"https://docs.google.com/spreadsheets/d/1UwJaPkuYh8rDk7YyjkquuqVz9rBsxe-g1GQIKRFcPYQ/edit#gid=0\", # SH\n",
    "        \"https://docs.google.com/spreadsheets/d/1lA2M2Ds9qmJrzqd18EFxWhplq5uO7oaC4nzVzzOmYws/edit#gid=0\", # AB\n",
    "        \"https://docs.google.com/spreadsheets/d/1R4xpYOAuLoqPfqooH_DxJjYjaXmeB4QXvOMXGvjokLg/edit#gid=0\", # MC\n",
    "        \"https://docs.google.com/spreadsheets/d/1hmYGD21eQozGTV-rY5m1KGsbgtGpQurK2tinyOTaUM4/edit#gid=0\", # SM\n",
    "        \"https://docs.google.com/spreadsheets/d/1f7-OAL1HE8ON-97Snd5HsUhnqV_zs6WRTaAHNBxU7W0/edit#gid=0\"] #, # SMi\n",
    "\n",
    "annotation_df = iaa_utilities.IAAv1SpreadsheetScheme.load_locally_fallback_to_web(\"iaa-v1.csv\", urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning the dataset\n",
    "\n",
    "There were a couple of superficial differences between the different annotators that we need to handle for the first round of IAA:\n",
    "\n",
    "1. some annotators left whole rows blank; and\n",
    "2. annotator paraphrase of definition was often left blank, as was the column for statistics. blank entries compare poorly on set-distance metrics so we will replace these with \"~*EMPTY*~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_values = pd.DataFrame(annotation_df.loc[:,'system_language':'op_statistics']).any(axis = 1)\n",
    "annotation_df = annotation_df[no_values]\n",
    "\n",
    "annotation_df.replace(\"^\\s$\", \"~*EMPTY*~\", inplace=True)\n",
    "\n",
    "for column in iaa_utilities.IAAv1SpreadsheetScheme.OPEN_CLASS_COLUMNS:\n",
    "    annotation_df[column] = annotation_df[column].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the relevant information\n",
    "\n",
    "Now that we've prepared the primary dataframe, we can easily extract smaller dataframes which facilitate the analysis.\n",
    "In particular, this function gives us a three-column DF with the `source_spreadsheet`, `key` (= paper identifier), and the target column,\n",
    "where we have aggregated all labels given in that column for that paper in the spreadsheet `source_spreadsheet` into a set.\n",
    "(Using a set means that each label will appear only once; using a `frozenset` makes it immutable.)\n",
    "\n",
    "This code appears in `iaa_utilities.py`\n",
    "\n",
    "    def extract_iaa_df_by_column_name(annotation_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract a three-column dataframe with `column_name` items grouped by `source_spreadsheet` and `key`.\"\"\"\n",
    "        return annotation_df[['source_spreadsheet', 'key', column_name]]\\\n",
    "            .groupby(['source_spreadsheet', 'key'])[column_name]\\\n",
    "            .apply(frozenset).reset_index()\n",
    "\n",
    "    def extract_records_for_nltk(iaa_df: pd.DataFrame) -> List[Tuple]:\n",
    "        \"\"\"The first column in the `to_records()` representation is an index, which we don't need for `nltk`.\"\"\"\n",
    "        return [(b, c, d) for _, b, c, d in iaa_df.to_records()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_iaa_df_by_column_name = iaa_utilities.extract_iaa_df_by_column_name\n",
    "extract_records_for_nltk = iaa_utilities.extract_records_for_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculating agreement\n",
    "\n",
    "We will first test our approach with a single column and MASI distance,\n",
    "before running it for all the columns for both MASI and Jaccard.\n",
    "In this example we focus on `criterion_paraphrase`, a mostly closed-class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Grab the three-column version\n",
    "criterion_paraphrase_df = extract_iaa_df_by_column_name(annotation_df, \"criterion_paraphrase\")\n",
    "# Define an AnnotationTask with our preferred distance metric\n",
    "criterion_paraphrase_task = AnnotationTask(distance=masi_distance)\n",
    "# Extract the records from our three-column dataframe and load them into the AnnotationTask\n",
    "criterion_paraphrase_task.load_array(extract_records_for_nltk(criterion_paraphrase_df))\n",
    "# See what we get for Krippendorff's alpha\n",
    "print(criterion_paraphrase_task.alpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we want to repeat the exercise for *all* of the closed-class columns **and** using both MASI and Jaccard distance.\n",
    "We'll store the three-column dataframes and resulting alpha values in a dict for easy access later.\n",
    "\n",
    "The following code appears in `iaa_utilities.py`, converted to a method for each spreadsheet format:\n",
    "\n",
    "    def run_closed_class_jaccard_and_masi(df: pd.DataFrame) -> Dict:\n",
    "        iaa_by_column = {column: {\"df\": extract_iaa_df_by_column_name(df, column)} for column in iaa_utilities.IAAv1SpreadsheetScheme.CLOSED_CLASS_COLUMNS}\n",
    "        for column in iaa_by_column:\n",
    "            task = AnnotationTask(distance=jaccard_distance)\n",
    "            task.load_array(extract_records_for_nltk(iaa_by_column[column]['df']))\n",
    "            iaa_by_column[column]['alpha_jaccard'] = task.alpha()\n",
    "            task = AnnotationTask(distance=masi_distance)\n",
    "            task.load_array(extract_records_for_nltk(iaa_by_column[column]['df']))\n",
    "            iaa_by_column[column]['alpha_masi'] = task.alpha()\n",
    "        return iaa_by_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_by_column = iaa_utilities.IAAv1SpreadsheetScheme.run_closed_class_jaccard_and_masi(annotation_df)\n",
    "print(iaa_by_column['criterion_paraphrase']['df'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_utilities.pretty_print_iaa_by_column(iaa_by_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This did reasonable things for our dev data in a strict-agreement mode, but we should also produce a version which relaxes some of the restrictions.\n",
    "In particular, we want to collapse categories which will be trivially different.\n",
    "For example, the \"other\" and \"multiple\" options which annotators could use, which include further details which will always differ.\n",
    "In these instances it is informative to know that annotators agreed that, e.g., the annotation scheme did not cover this paper (choosing \"other\").\n",
    "\n",
    "We also want to leverage hierarchical structure in the annotation scheme when possible, such as for the `criterion_paraphrase` column.\n",
    "\n",
    "## Broad Agreement\n",
    "\n",
    "We will call the exact-matching (at the string level) version of agreement which we have used so far *narrow* and now define *broad* agreement.\n",
    "Broad agreement uses the natural hierarchies in the annotation scheme to group elements together which we might want to consider as equivalent.\n",
    "\n",
    "For example, if two annotators disagree about the output type of a system, with one saying *text: paragraph* and the other saying *text: document*,\n",
    "we want to penalize this less than if one of them were to say *multi-modal* instead.\n",
    "\n",
    "### Input/Output Columns\n",
    "\n",
    "For the `system_input` and `system_output` columns, we will consider the following equivalence classes\n",
    "\n",
    "* text = {text: subsentential units of text, text: sentence, text: paragraph, text: document, text: dialogue, text: other (please specify)},\n",
    "* multiple = {all variations of *multiple (list all)*}, and\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "with all other allowed labels belonging individually to their own equivalence class containing only one element (i.e. raw data = {*raw data*})\n",
    "In the narrow agreement calculations, each element in an equivalence class differs from all others with a nominal distance metric (i.e. identical strings are distance 0 and all others are distance 1 from each other).\n",
    "\n",
    "\n",
    "### Task Column\n",
    "\n",
    "For the `system_task` column, we use the following equivalence classes\n",
    "\n",
    "* multiple = {all variations of *multiple (list all)*} and\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "with all other allowed labels belonging individually to their own equivalence class containing only one element (i.e. aggregation = {*aggregation*})\n",
    "In the narrow agreement calculations, each element in an equivalence class differs from all others with a nominal distance metric (i.e. identical strings are distance 0 and all others are distance 1 from each other).\n",
    "\n",
    "### Paraphrase of Criterion Name Column\n",
    "\n",
    "For the `criterion_paraphrase` column, we use two sets of equivalence classes:\n",
    "one for simple string-level differences related to annotator-specified details as in the above cases and\n",
    "another based on the hierarchy of criteria.\n",
    "\n",
    "#### String-level equivalence classes\n",
    "\n",
    "* Detectability of Text Property (specify property here) = {all variations of *Detectability of Text Property*}\n",
    "* Effect on listener (specify effect here) = {all variations of *Effect on listener*}\n",
    "* Inferrability of Speaker Stance (specify object of stance here) = {all variations of *Inferrability of Speaker Stance*}\n",
    "* Inferrability of Speaker Trait (specify trait here) = {all variations of *Inferrability of Speaker Trait*}\n",
    "\n",
    "#### Hierarchy-based equivalence classes\n",
    "\n",
    "* Quantitative Criteria = {*Quantitative Criteria*, *Input Surface Form Retention*, *Input Content Retention*}\n",
    "* Quality of Surface Form = {*Quality of Surface Form*, *Correctness of Surface Form*, *Grammaticality*, *Spelling Accuracy*, *Quality of Expression ('well-written')*, *Speech Quality*, *Aesthetic Quality of Surface Form*, *Appropriateness of form given context*}\n",
    "* Quality of Content = {*Quality of Content*, *Correctness of Content*, *Correctness relative to input*, *Correctness relative to External Reference*, *Answerability from input*, *Adequacy/Appropriateness*, *Adequacy (does text have all and only relevant content given the input)*, *Adequacy Precision (does text have only relevant content given the input)*, *Adequacy Recall (does text have all relevant content given the input)*, *Appropriateness given context*, *Quality of Content*, *Informativeness*, *Too much / not enough information*}\n",
    "* Quality of text as a whole = {*Quality of text as a whole*, *Coherence*, *Cohesiveness*, *Well-orderedness*, *Referent Resolvability*, *Complexity*, *Complexity/Simplicity of Form*, *Complexity/Simplicity of Content*, *Technicality/requires subject expertise*, *Naturalness*, *Naturalness (=likelihood)*, *Naturalness (form)*, *Naturalness (content)*, *Conversationality*, *Ease of Communication*, *Readability*, *Fluency*, *Clarity*, *Understandability*, *Nonredundancy*, *Nonredundancy (form)*, *Nonredundancy (content)*, *Vagueness/Specificity*, *Vagueness/Specificity (form)*, *Vagueness/Specificity (content)*, *Variedness*, *Variedness (form)*, *Variedness (content)*, *Originality*, *Originality (form)*, *Originality (content)*,  *Intended Property*, *Detectability of Text Property*}\n",
    "* Extralinguistic Quality = {*Extralinguistic Quality*, *Criteria related to Listener/Reader*, *Effect on listener*, *Inferrability of Speaker Stance*, *Inferrability of Speaker Trait*, *Learnability*, *Visualisability*, *Humanlikeness*, *Humanlikeness (form)*, *Humanlikeness (content)*, *Usefulness (nonspecific)*, *Usefulness for task / information need*, *Criteria related to system*, *User Satisfaction*, *Usability*}\n",
    "\n",
    "### Form of Response Elicitation\n",
    "\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "### Performing the updates and the calculations\n",
    "\n",
    "We work with a fresh copy of the annotation_df so that the original data is still accessible in the notebook.\n",
    "For each of the columns where *other (please specify)* is a valid annotation, we replace any annotation beginning with \"other\" with \"other\":\n",
    "we are collapsing the distinctions created by the further specifications.\n",
    "We do the same thing for annotations of *multiple (list all)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_df = annotation_df.copy(deep = True)\n",
    "for column in (\"system_input\", \"system_output\", \"system_task\", \"op_form\"):\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Oo]ther.*\", \"other\")\n",
    "broad_anno_df['system_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in (\"system_input\", \"system_output\"):\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Mm]ultiple.*\", \"multiple\")\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Tt]ext:.*\", \"text\")\n",
    "broad_anno_df['system_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When it comes to the `criterion_paraphrase` column, however, we can take either of the two approaches described above.\n",
    "We will create one copy of the broad annotation dataframe for each of them and then apply our fixes to the copies.\n",
    "\n",
    "For the version focused only on discrepancies caused by 'please specify' lists,\n",
    "we can use the same kind of approach we used earlier for 'other' and 'multiple':\n",
    "look for the keyphrase at the beginning of the cell and remove any other cell contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_string_df = broad_anno_df.copy(deep = True)\n",
    "for string_prefix in (\"Detectability of Text Property\", \"Effect on listener\", \"Inferrability of Speaker Stance\", \"Inferrability of Speaker Trait\"):\n",
    "    broad_anno_string_df['criterion_paraphrase'] = broad_anno_string_df['criterion_paraphrase'].str.replace(f\"^{string_prefix}.*\", string_prefix)\n",
    "broad_anno_string_df['criterion_paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the hierarchical version, we need to define the hierarchy as a replacement dictionary first.\n",
    "Here we define a dictionary where the key is the 'higher-level' which we will use as a replacement for each of the values associated with it (the 'lower-level')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_hierarchical_df = broad_anno_df.copy(deep = True)\n",
    "hierarchy_dict = iaa_utilities.IAAv1SpreadsheetScheme.HIERARCHY_DICT\n",
    "\n",
    "for higher_level in hierarchy_dict:\n",
    "    for lower_level in hierarchy_dict[higher_level]:\n",
    "        broad_anno_hierarchical_df['criterion_paraphrase'] = broad_anno_hierarchical_df['criterion_paraphrase'].str.replace(f\"^{lower_level}.*\", higher_level)\n",
    "broad_anno_hierarchical_df['criterion_paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can then repeat our calculations of $\\alpha$ using the broad versions of the spreadsheet.\n",
    "\n",
    "#### For the string-based calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_string_dict = iaa_utilities.IAAv1SpreadsheetScheme.run_closed_class_jaccard_and_masi(broad_anno_string_df)\n",
    "iaa_utilities.pretty_print_iaa_by_column(broad_string_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_string_dict['system_output']['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "at = AnnotationTask(data = extract_records_for_nltk(broad_string_dict['system_output']['df']), distance=jaccard_distance)\n",
    "at.alpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### For the hierarchical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_hier_dict = iaa_utilities.IAAv1SpreadsheetScheme.run_closed_class_jaccard_and_masi(broad_anno_hierarchical_df)\n",
    "iaa_utilities.pretty_print_iaa_by_column(broad_hier_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pairwise interannotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "iaa_utilities.IAAv1SpreadsheetScheme.print_absolute_agreement(annotation_df, iaa_by_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pair-wise agreement on the broad string-based annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_utilities.IAAv1SpreadsheetScheme.print_absolute_agreement(broad_anno_string_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise agreement on the broad hierarchical annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_utilities.IAAv1SpreadsheetScheme.print_absolute_agreement(broad_anno_hierarchical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement tables for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in iaa_utilities.IAAv1SpreadsheetScheme.ALL_DATA_COLUMNS:\n",
    "    display(column)\n",
    "    display(extract_iaa_df_by_column_name(annotation_df, column).pivot(index=\"key\", columns=\"source_spreadsheet\", values=column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pair-wise agreement on the broad hierarchical annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_utilities.IAAv1SpreadsheetScheme.print_absolute_agreement(broad_anno_hierarchical_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
