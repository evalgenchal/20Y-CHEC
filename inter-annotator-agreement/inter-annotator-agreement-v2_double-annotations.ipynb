{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inter-annotator Agreement v2.0 for INLG 2020\n",
    "\n",
    "The annotation team formalized our annotation guidelines in July 2020.\n",
    "We then annotated 10 papers from ACL 2020 according to these new guidelines to get an idea of the degree of inter-annotator agreement;\n",
    "this gives us a sense of how reliable the new guidelines and our annotations are.\n",
    "The results were disappointing so we iterated on our guidelines and annotation spreadsheets and did another round of 10 papers.\n",
    "This document reports on our IAA for these 10 papers.\n",
    "\n",
    "In this notebook, we import data from our spreadsheets and do a bit of preprocessing so that we can calculate IAA easily using `nltk`.\n",
    "\n",
    "We calculate [Krippendorff's alpha]() using [MASI distance]() and [Jaccard distance]().\n",
    "We also include raw pair-wise agreement scores.\n",
    "\n",
    "Note that we are not doing any hypothesis testing here, so you will not see any significance scores.\n",
    "These are strictly descriptive statistics.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Our original annotations were collected using [Google Sheets]() so we used `gspread` to interact with Google Sheets, `nltk` to calculate $\\alpha$, and `pandas` to manage our data.\n",
    "These spreadsheets are not public, but the data from them is released in the CSV files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.metrics import edit_distance, jaccard_distance\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import iaa_utilities\n",
    "\n",
    "\n",
    "# URLs for the actual spreadsheets\n",
    "urls = [\"https://docs.google.com/spreadsheets/d/17SXR1PMFiavwNUgg3q6EDHWuIiuK2jGaFgb8N-EUukA/edit\",\n",
    "        \"https://docs.google.com/spreadsheets/d/18FhVQ0h1kRYuoSGF1DVHMyu-Ag3ZHsDS2ppOIf8z3mc/edit\",\n",
    "        \"https://docs.google.com/spreadsheets/d/1gRyvyJT-6rnMhPyvSD82JJHAXEaL-7e2IjHkFPV8kX0/edit\",\n",
    "        \"https://docs.google.com/spreadsheets/d/1DmnT-4ab4wCh9NczY7IEolussQhYXZu5q7mP72Kv0rY/edit\"]\n",
    "\n",
    "\n",
    "# If you want to force loading from the web instead of trying to load locally, uncomment the next line and comment the one after that.\n",
    "# annotation_df = iaa_utilities.IAAv2SpreadsheetScheme.prepare_df_from_google_sheets(urls)\n",
    "annotation_df = iaa_utilities.IAAv2SpreadsheetScheme.load_locally_fallback_to_web(\"iaa-v2_double-annotations.csv\", urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning the dataset\n",
    "\n",
    "We keep the code used to clean up superficial differences between the different annotators that we needed to handle for the first round of IAA:\n",
    "\n",
    "1. some annotators left whole rows blank; and\n",
    "2. annotator paraphrase of definition was often left blank, as was the column for statistics. blank entries compare poorly on set-distance metrics so we will replace these with \"~*EMPTY*~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_values = pd.DataFrame(annotation_df.loc[:,'system_language':'op_statistics']).any(axis = 1)\n",
    "annotation_df = annotation_df[no_values]\n",
    "\n",
    "annotation_df.replace(\"^\\s$\", \"~*EMPTY*~\", inplace=True)\n",
    "\n",
    "for column in iaa_utilities.IAAv2SpreadsheetScheme.OPEN_CLASS_COLUMNS:\n",
    "    annotation_df[column] = annotation_df[column].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the second round of IAA, in addition to updating the guidelines, we updated the spreadsheet to have dropdown menus for the criteria names and other columns.\n",
    "For the `criterion_paraphrase` column, this included the enumeration from the guidelines, as well as value-initial hyphens to get some degree of indentation indicative of the overall hierarchy.\n",
    "We therefore need to do a bunch of normalization for the criterion_paraphrase column to remove the hyphens and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "annotation_df['criterion_paraphrase'] = annotation_df['criterion_paraphrase'].str.lower().str.replace(\"-\", \"\").str.replace(\"[0123456789a-z\\/]+\\.\", \"\").str.replace(\"\\s+\", \" \").str.replace(\";\", \",\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also need to deal with columns where `multiple (please specify):` is a valid value, so that the likelihood of spurious differences is as low as possible.\n",
    "Ideally we should standardise the order of the values listed, but we did not do that for the initial INLG submission (hence the empty code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the relevant information\n",
    "\n",
    "Now that we've prepared the primary dataframe, we can easily extract smaller dataframes which facilitate the analysis.\n",
    "In particular, this function gives us a three-column DF with the `source_spreadsheet`, `key` (= paper identifier), and the target column,\n",
    "where we have aggregated all labels given in that column for that paper in the spreadsheet `source_spreadsheet` into a set.\n",
    "(Using a set means that each label will appear only once; using a `frozenset` makes it immutable.)\n",
    "\n",
    "This code appears in `iaa_utilities.py`\n",
    "\n",
    "    def extract_iaa_df_by_column_name(annotation_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract a three-column dataframe with `column_name` items grouped by `source_spreadsheet` and `key`.\"\"\"\n",
    "        return annotation_df[['source_spreadsheet', 'key', column_name]]\\\n",
    "            .groupby(['source_spreadsheet', 'key'])[column_name]\\\n",
    "            .apply(frozenset).reset_index()\n",
    "\n",
    "    def extract_records_for_nltk(iaa_df: pd.DataFrame) -> List[Tuple]:\n",
    "        \"\"\"The first column in the `to_records()` representation is an index, which we don't need for `nltk`.\"\"\"\n",
    "        return [(b, c, d) for _, b, c, d in iaa_df.to_records()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "extract_iaa_df_by_column_name = iaa_utilities.extract_iaa_df_by_column_name\n",
    "extract_records_for_nltk = iaa_utilities.extract_records_for_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculating agreement\n",
    "\n",
    "We use the same setup for calculating Krippendorff's alpha with Jaccard distance and MASI distance for the closed-class columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_by_column = iaa_utilities.IAAv2SpreadsheetScheme.run_closed_class_jaccard_and_masi(annotation_df)\n",
    "print(iaa_by_column['criterion_paraphrase']['df'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iaa_utilities.pretty_print_iaa_by_column(iaa_by_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This does reasonable things for our dev data in a strict-agreement mode, but we should also produce a version which relaxes some of the restrictions.\n",
    "We should also do one for open-class columns with a different distance measure.\n",
    "\n",
    "## Broad Agreement\n",
    "\n",
    "We will call the exact-matching (at the string level) version of agreement which we have used so far *narrow* and now define *broad* agreement.\n",
    "Broad agreement uses the natural hierarchies in the annotation scheme to group elements together which we might want to consider as equivalent.\n",
    "\n",
    "For example, if two annotators disagree about the output type of a system, with one saying *text: paragraph* and the other saying *text: document*,\n",
    "we want to penalize this less than if one of them were to say *multi-modal* instead.\n",
    "\n",
    "### Input/Output Columns\n",
    "\n",
    "For the `system_input` and `system_output` columns, we will consider the following equivalence classes\n",
    "\n",
    "* text = {text: subsentential units of text, text: sentence, text: paragraph, text: document, text: dialogue, text: other (please specify)},\n",
    "* multiple = {all variations of *multiple (list all)*}, and\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "with all other allowed labels belonging individually to their own equivalence class containing only one element (i.e. raw data = {*raw data*})\n",
    "In the narrow agreement calculations, each element in an equivalence class differs from all others with a nominal distance metric (i.e. identical strings are distance 0 and all others are distance 1 from each other).\n",
    "\n",
    "\n",
    "### Task Column\n",
    "\n",
    "For the `system_task` column, we use the following equivalence classes\n",
    "\n",
    "* multiple = {all variations of *multiple (list all)*} and\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "with all other allowed labels belonging individually to their own equivalence class containing only one element (i.e. aggregation = {*aggregation*})\n",
    "In the narrow agreement calculations, each element in an equivalence class differs from all others with a nominal distance metric (i.e. identical strings are distance 0 and all others are distance 1 from each other).\n",
    "\n",
    "### Paraphrase of Criterion Name Column\n",
    "\n",
    "For the `criterion_paraphrase` column, we use two sets of equivalence classes:\n",
    "one for simple string-level differences related to annotator-specified details as in the above cases and\n",
    "another based on the hierarchy of criteria.\n",
    "\n",
    "#### String-level equivalence classes\n",
    "\n",
    "* Detectability of Text Property (specify property here) = {all variations of *Detectability of Text Property*}\n",
    "* Effect on listener (specify effect here) = {all variations of *Effect on listener*}\n",
    "* Inferrability of Speaker Stance (specify object of stance here) = {all variations of *Inferrability of Speaker Stance*}\n",
    "* Inferrability of Speaker Trait (specify trait here) = {all variations of *Inferrability of Speaker Trait*}\n",
    "\n",
    "#### Hierarchy-based equivalence classes\n",
    "\n",
    "These can be read in version 2.0 of the annotation guidelines.\n",
    "We use all the immediate children of `Quality of outputs` as the top level categories and map all of their children to them.\n",
    "This gives us four equivalence classes:\n",
    "\n",
    "* `Quality of outputs` (containing only itself),\n",
    "* `Correctness of outputs`,\n",
    "* `Goodness of outputs (excluding correctness)`, and\n",
    "* `Feature-type criteria`\n",
    "\n",
    "### Form of Response Elicitation\n",
    "\n",
    "* other = {all variations of *other (please specify)*}\n",
    "\n",
    "### Performing the updates and the calculations\n",
    "\n",
    "We work with a fresh copy of the annotation_df so that the original data is still accessible in the notebook.\n",
    "For each of the columns where *other (please specify)* is a valid annotation, we replace any annotation beginning with \"other\" with \"other:\n",
    "we are collapsing the distinctions created by the further specifications.\n",
    "We do the same thing for annotations of *multiple (list all)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_df = annotation_df.copy(deep = True)\n",
    "for column in (\"system_input\", \"system_output\", \"system_task\", \"op_form\"):\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Oo]ther.*\", \"other\")\n",
    "broad_anno_df['system_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in (\"system_input\", \"system_output\"):\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Mm]ultiple.*\", \"multiple\")\n",
    "    broad_anno_df[column] = broad_anno_df[column].str.replace(\"^[Tt]ext:.*\", \"text\")\n",
    "broad_anno_df['system_input']\n",
    "\n",
    "# broad_anno_df['criterion_paraphrase'] = broad_anno_df['criterion_paraphrase'].str.replace(\"^[Mm]ultiple.*\", \"multiple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When it comes to the `criterion_paraphrase` column, however, we can take either of the two approaches described above.\n",
    "We will create one copy of the broad annotation dataframe for each of them and then apply our fixes to the copies.\n",
    "\n",
    "For the version focused only on discrepancies caused by 'please specify' lists,\n",
    "we can use the same kind of approach we used earlier for 'other' and 'multiple':\n",
    "look for the keyphrase at the beginning of the cell and remove any other cell contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_string_df = broad_anno_df.copy(deep = True)\n",
    "for string_prefix in (\"Text Property\", \"Detectability of controlled feature\", \"Effect on reader/listener\", \"Inferrability of speaker/author stance\", \"Inferrability of speaker/author trait\"):\n",
    "    broad_anno_string_df['criterion_paraphrase'] = broad_anno_string_df['criterion_paraphrase'].str.replace(f\"{string_prefix}.*\", string_prefix, case=False)\n",
    "\n",
    "broad_anno_string_df['criterion_paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the hierarchical version, we need to define the hierarchy as a replacement dictionary first.\n",
    "Here we define a dictionary where the key is the 'higher-level' which we will use as a replacement for each of the values associated with it (the 'lower-level')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_anno_hierarchical_df = broad_anno_df.copy(deep = True)\n",
    "\n",
    "hierarchy_dict = iaa_utilities.IAAv2SpreadsheetScheme.HIERARCHY_DICT\n",
    "\n",
    "for higher_level in hierarchy_dict:\n",
    "    for lower_level in hierarchy_dict[higher_level]:\n",
    "        broad_anno_hierarchical_df['criterion_paraphrase'] = broad_anno_hierarchical_df['criterion_paraphrase'].str.replace(f\".*{re.escape(lower_level)}.*\", higher_level, case = False)\n",
    "broad_anno_hierarchical_df['criterion_paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can then repeat our calculations of $\\alpha$ using the broad versions of the spreadsheet.\n",
    "\n",
    "#### For the string-based calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_string_dict = iaa_utilities.IAAv2SpreadsheetScheme.run_closed_class_jaccard_and_masi(broad_anno_string_df)\n",
    "iaa_utilities.pretty_print_iaa_by_column(broad_string_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### For the hierarchical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "broad_hier_dict = iaa_utilities.IAAv2SpreadsheetScheme.run_closed_class_jaccard_and_masi(broad_anno_hierarchical_df)\n",
    "iaa_utilities.pretty_print_iaa_by_column(broad_hier_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pairwise interannotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_absolute_agreement(dataframe: pd.DataFrame, iaa_by_column_dict: Optional[Dict] = None) -> None:\n",
    "    if iaa_by_column_dict is None:\n",
    "        iaa_by_column_dict = iaa_utilities.IAAv2SpreadsheetScheme.run_closed_class_jaccard_and_masi(dataframe)\n",
    "    for column in iaa_utilities.IAAv2SpreadsheetScheme.CLOSED_CLASS_COLUMNS:\n",
    "        df = iaa_by_column_dict[column]['df']\n",
    "        print(f\"Interannotator agreement for {column}\")\n",
    "        annotator_list = dataframe.source_spreadsheet.unique()\n",
    "        print(\" \\t\" + \"\\t\".join([str(annotator) for annotator in annotator_list]))\n",
    "        for a1 in annotator_list:\n",
    "            a1_vals = list(df[df.source_spreadsheet == a1][column])\n",
    "            print(f\"{a1}\", end=\"\\t\")\n",
    "            pairwise_agreements = []\n",
    "            for a2 in annotator_list:\n",
    "                a2_vals = list(df[df.source_spreadsheet == a2][column])\n",
    "                agreement_sum = 0\n",
    "                for a1_val, a2_val in zip(a1_vals, a2_vals):\n",
    "                    agreement_sum += 1 - jaccard_distance(a1_val, a2_val)\n",
    "                pairwise_agreements.append(agreement_sum/min(len(a1_vals), len(a2_vals)))\n",
    "                print(f\"{pairwise_agreements[-1]:.2f}\", end=\"\\t\")\n",
    "            print(f\"\\t{(sum(pairwise_agreements) - 1)/(len(pairwise_agreements) - 1):.2f}\")\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "print_absolute_agreement(annotation_df, iaa_by_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise agreement on the broad string-based annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_absolute_agreement(broad_anno_string_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise agreement on the broad hierarchical annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_absolute_agreement(broad_anno_hierarchical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Agreement tables for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in iaa_utilities.IAAv2SpreadsheetScheme.ALL_DATA_COLUMNS:\n",
    "    display(column)\n",
    "    display(extract_iaa_df_by_column_name(annotation_df, column).pivot(index=\"key\", columns=\"source_spreadsheet\", values=column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in iaa_utilities.IAAv2SpreadsheetScheme.ALL_DATA_COLUMNS:\n",
    "    display(column)\n",
    "    display(extract_iaa_df_by_column_name(broad_anno_hierarchical_df, column).pivot(index=\"key\", columns=\"source_spreadsheet\", values=column))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
